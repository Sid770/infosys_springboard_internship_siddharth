{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\sidhe\\Downloads\\cleaned_combined_dataset.xlsx\"\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Specify the columns for processing\n",
    "job_description_col = 'job_description'\n",
    "transcript_col = 'transcript'\n",
    "resume_col = 'resume'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing The Bert Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67f35a8967b4955854a5f1dc888730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sidhe\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd75c2086d44c71a442197f3b2cfb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d15ba8fcd274f0190f9d4524e75e0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6a88fcdfd441cda79f9a91ffe0b6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2decbc4ea0584fe988bdb68d240d3260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the columns for processing\n",
    "job_description_col = 'job_description'\n",
    "transcript_col = 'transcript'\n",
    "resume_col = 'resume'\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to get BERT embeddings and Process text columns with BERT embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the [CLS] token embedding as a summary of the text\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0).numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "# Process text columns with BERT embeddings\n",
    "def process_with_bert(column_name):\n",
    "    embeddings = []\n",
    "    for text in data[column_name].fillna(''):\n",
    "        embedding = get_bert_embeddings(text)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getting the enbeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for job descriptions...\n",
      "Generating BERT embeddings for transcripts...\n",
      "Generating BERT embeddings for resumes...\n",
      "Preparing for Word2Vec embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating BERT embeddings for job descriptions...\")\n",
    "data[f'{job_description_col}_bert'] = process_with_bert(job_description_col)\n",
    "\n",
    "print(\"Generating BERT embeddings for transcripts...\")\n",
    "data[f'{transcript_col}_bert'] = process_with_bert(transcript_col)\n",
    "\n",
    "print(\"Generating BERT embeddings for resumes...\")\n",
    "data[f'{resume_col}_bert'] = process_with_bert(resume_col)\n",
    "\n",
    "# Prepare for Word2Vec embeddings\n",
    "print(\"Preparing for Word2Vec embeddings...\")\n",
    "text_data = data[[job_description_col, transcript_col, resume_col]].fillna('').values.flatten()\n",
    "tokenized_data = [text.split() for text in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Word2Vec model and function to get Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get Word2Vec embeddings\n",
    "def get_word2vec_embeddings(text):\n",
    "    words = text.split()\n",
    "    embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process text columns with Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def process_with_word2vec(column_name):\n",
    "    embeddings = []\n",
    "    for text in data[column_name].fillna(''):\n",
    "        embedding = get_word2vec_embeddings(text)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Word2Vec embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Word2Vec embeddings for job descriptions...\n",
      "Generating Word2Vec embeddings for transcripts...\n",
      "Generating Word2Vec embeddings for resumes...\n",
      "Processed data saved to C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Word2Vec embeddings for job descriptions...\")\n",
    "data[f'{job_description_col}_word2vec'] = process_with_word2vec(job_description_col)\n",
    "\n",
    "print(\"Generating Word2Vec embeddings for transcripts...\")\n",
    "data[f'{transcript_col}_word2vec'] = process_with_word2vec(transcript_col)\n",
    "\n",
    "print(\"Generating Word2Vec embeddings for resumes...\")\n",
    "data[f'{resume_col}_word2vec'] = process_with_word2vec(resume_col)\n",
    "\n",
    "# Save processed data\n",
    "output_file = r'C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings.xlsx'\n",
    "data.to_excel(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Glove Model for embeddings (Based on research on internet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model...\n",
      "Embedding column: job_description\n",
      "Embedding column: transcript\n",
      "Embedding column: resume\n",
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                           job_description_embedding  \\\n",
      "0  [-0.06838082352941176, 0.16142741176470587, 0....   \n",
      "1  [-0.07749826666666665, 0.18295106666666663, 0....   \n",
      "2  [-0.07265462499999999, 0.17151662499999998, 0....   \n",
      "3  [-0.07265462499999999, 0.17151662499999998, 0....   \n",
      "4  [-0.06838082352941176, 0.16142741176470587, 0....   \n",
      "\n",
      "                                transcript_embedding  \\\n",
      "0  [-0.07743272709677428, 0.14780251290322574, 0....   \n",
      "1  [-0.09439831557496364, 0.12709692096069872, 0....   \n",
      "2  [-0.09132300704500977, 0.15501444794520533, 0....   \n",
      "3  [-0.09758575254010692, 0.16322203890374315, 0....   \n",
      "4  [-0.08310437412199626, 0.1506663049907578, 0.1...   \n",
      "\n",
      "                                    resume_embedding  \n",
      "0  [-0.09252163974358976, 0.11051320512820514, 0....  \n",
      "1  [-0.13412059818731115, 0.11648161117824783, 0....  \n",
      "2  [-0.12101620384615384, 0.16451516449704148, 0....  \n",
      "3  [-0.1170698708206688, 0.14475598480243168, 0.0...  \n",
      "4  [-0.11942216625916875, 0.14034793887530575, 0....  \n",
      "File saved to C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings_glove_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings (assuming you have the pre-trained GloVe file, e.g., 'glove.6B.100d.txt')\n",
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading GloVe model...\")\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "# Function to get GloVe vector for a word (returns zero vector if word is not in the GloVe model)\n",
    "def get_glove_vector(word, glove_model, vector_size=100):\n",
    "    return glove_model.get(word, np.zeros(vector_size))\n",
    "\n",
    "# Load your Excel data\n",
    "csv_file = r\"B:\\OneDrive - Amity University\\Desktop\\Tower Research\\cleaned_combined_dataset.xlsx\"\n",
    "df = pd.read_excel(csv_file) \n",
    "\n",
    "# Load GloVe embeddings (replace with path to your GloVe file)\n",
    "glove_model = load_glove_model(\"B:\\OneDrive - Amity University\\Desktop\\glove.6B\\glove.6B.100d.txt\")\n",
    "\n",
    "# Select columns to apply GloVe embeddings\n",
    "columns_to_embed = ['job_description', 'transcript', 'resume']  # Replace with the columns you want to embed\n",
    "\n",
    "# Embed the columns\n",
    "for column in columns_to_embed:\n",
    "    print(f\"Embedding column: {column}\")\n",
    "    df[column + '_embedding'] = df[column].apply(lambda x: np.mean([get_glove_vector(word, glove_model) for word in str(x).split()], axis=0))\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save the dataframe with the embeddings to an Excel file\n",
    "output_file = r'C:\\Users\\sidhe\\Downloads\\processed_dataset_with_embeddings_glove_2.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, index=False, sheet_name='Embeddings')\n",
    "\n",
    "print(f\"File saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre proccesing the csv of embeddings for futther work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                                job_description_bert  \\\n",
      "0  [-0.184033975, -0.102659881, -0.133955404, -0....   \n",
      "1  [-0.103227332, -0.115133069, -0.304571718, -0....   \n",
      "2  [-0.240335613, -0.219893694, -0.0979446471, 0....   \n",
      "3  [-0.0361063704, -0.163667649, -0.22443682, -0....   \n",
      "4  [-0.190464601, -0.202941269, -0.327967256, -0....   \n",
      "\n",
      "                                     transcript_bert  \\\n",
      "0  [-0.569111705, -0.543804824, -0.141024753, 0.1...   \n",
      "1  [-0.545914114, -0.295345664, -0.202920973, 0.0...   \n",
      "2  [-0.810874343, -0.253732115, -0.495483011, 0.3...   \n",
      "3  [-0.165312082, -0.668395579, -0.659501672, -0....   \n",
      "4  [-0.518009365, -0.229858547, -0.247252136, 0.2...   \n",
      "\n",
      "                                         resume_bert  \\\n",
      "0  [-1.01753056, -0.191111356, -0.31804511, -0.28...   \n",
      "1  [-0.490350217, 0.18670845, -0.368963569, 0.060...   \n",
      "2  [-1.03070152, 0.370450199, -0.449870557, -0.04...   \n",
      "3  [-0.4157016, -0.29293224, -0.16800818, -0.1828...   \n",
      "4  [-0.803099036, 0.252510846, -0.238545656, 0.10...   \n",
      "\n",
      "                            job_description_word2vec  \\\n",
      "0  [0.5068023, -0.12546456, 0.7389149, -0.6894486...   \n",
      "1  [1.0229793, 0.05989623, 1.4022692, -0.75934297...   \n",
      "2  [0.9270255, 0.0133434, 1.7455078, -0.71988434,...   \n",
      "3  [0.21010208, 0.41417825, 1.2362776, -1.1720171...   \n",
      "4  [0.85659266, -0.10477389, 1.6043124, -0.852205...   \n",
      "\n",
      "                                 transcript_word2vec  \\\n",
      "0  [0.01075406, 0.7207075, 0.16426691, -0.1420609...   \n",
      "1  [0.09987153, 0.7372747, 0.06237955, -0.1804453...   \n",
      "2  [0.1902774, 0.549441, 0.4270309, 0.02757834, 0...   \n",
      "3  [0.16780676, 0.743048, 0.17947781, -0.1912264,...   \n",
      "4  [0.3677691, 0.41351652, 0.10227631, -0.1352370...   \n",
      "\n",
      "                                     resume_word2vec  \n",
      "0  [-0.6371148, 0.80693257, 0.11435751, -0.483057...  \n",
      "1  [0.12514763, 0.6327575, 0.63175255, -0.3424733...  \n",
      "2  [0.20390636, 0.4078928, 0.9611415, -0.03728159...  \n",
      "3  [-0.4162535, 1.015284, 0.7123761, -0.4110522, ...  \n",
      "4  [0.03200936, 0.61920214, 0.6882263, -0.1802743...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel('B:\\OneDrive - Amity University\\Desktop\\Tower Research\\processed_dataset_with_embeddings.xlsx')\n",
    "\n",
    "# Example: Assuming the embeddings are in columns 6, 7, and 8 (adjust according to your actual column names)\n",
    "embedding_columns = ['job_description_bert', 'transcript_bert', 'resume_bert','job_description_word2vec','transcript_word2vec','resume_word2vec']  # replace with your actual column names\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "    return [float(num) for num in embedding_list]  # convert each number to float\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV if needed\n",
    "df.to_csv('modified_embeddings_bert_word2vec.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre proccesing the glove embeddings for further work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "               source_file source_sheet  \\\n",
      "0  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "1  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "2  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "3  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "4  ./dataset\\dataset1.xlsx       Sheet1   \n",
      "\n",
      "                           job_description_embedding  \\\n",
      "0  [-0.06838082, 0.16142741, 0.08740371, -0.03329...   \n",
      "1  [-0.07749827, 0.18295107, 0.09905753, -0.03773...   \n",
      "2  [-0.07265462, 0.17151662, 0.09286644, -0.03537...   \n",
      "3  [-0.07265462, 0.17151662, 0.09286644, -0.03537...   \n",
      "4  [-0.06838082, 0.16142741, 0.08740371, -0.03329...   \n",
      "\n",
      "                                transcript_embedding  \\\n",
      "0  [-0.0774327271, 0.147802513, 0.225390829, -0.0...   \n",
      "1  [-0.0943983156, 0.127096921, 0.184848412, -0.0...   \n",
      "2  [-0.09132301, 0.15501445, 0.18994288, -0.10643...   \n",
      "3  [-0.0975857525, 0.163222039, 0.178800796, -0.1...   \n",
      "4  [-0.08310437, 0.1506663, 0.1978036, -0.0860924...   \n",
      "\n",
      "                                    resume_embedding  \n",
      "0  [-0.0925216397, 0.110513205, 0.116988403, -0.0...  \n",
      "1  [-0.134120598, 0.116481611, 0.0549653474, 0.00...  \n",
      "2  [-0.121016204, 0.164515164, 0.0728409408, 0.02...  \n",
      "3  [-0.11706987, 0.14475598, 0.05192951, -0.01342...  \n",
      "4  [-0.119422166, 0.140347939, 0.0922491961, 0.01...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel('B:\\OneDrive - Amity University\\Desktop\\Tower Research\\processed_dataset_with_embeddings_glove_2.xlsx')\n",
    "\n",
    "# Example: Assuming the embeddings are in columns 6, 7, and 8 (adjust according to your actual column names)\n",
    "embedding_columns = ['job_description_embedding', 'transcript_embedding','resume_embedding']  # replace with your actual column names\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "    return [float(num) for num in embedding_list]  # convert each number to float\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV if needed\n",
    "df.to_csv('modified_embeddings_glove.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training the 3 types of model embeddings in XG BOOST and ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN model...\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "82/82 [==============================] - 3s 14ms/step - loss: 0.4026 - accuracy: 0.8123 - val_loss: 0.3041 - val_accuracy: 0.8231\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2844 - accuracy: 0.8415 - val_loss: 0.2967 - val_accuracy: 0.8123\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 0.2510 - accuracy: 0.8573 - val_loss: 0.2845 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 0.2372 - accuracy: 0.8669 - val_loss: 0.3009 - val_accuracy: 0.8308\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2274 - accuracy: 0.8665 - val_loss: 0.3273 - val_accuracy: 0.8092\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2118 - accuracy: 0.8819 - val_loss: 0.2989 - val_accuracy: 0.8185\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.2081 - accuracy: 0.8835 - val_loss: 0.3114 - val_accuracy: 0.8123\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 1s 8ms/step - loss: 0.1935 - accuracy: 0.8919 - val_loss: 0.3420 - val_accuracy: 0.7954\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.1799 - accuracy: 0.9042 - val_loss: 0.3188 - val_accuracy: 0.8138\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.1629 - accuracy: 0.9188 - val_loss: 0.3502 - val_accuracy: 0.8123\n",
      "21/21 [==============================] - 0s 3ms/step\n",
      "ANN Model Accuracy: 0.8123\n",
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [23:32:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models saved successfully as 'ann_model.h5', 'xgb_model.json', and 'ann_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "def process_embedding_string(embedding_str):\n",
    "    \"\"\"Convert string representation of embeddings to numpy array\"\"\"\n",
    "    try:\n",
    "        # Convert string representation of list to actual list\n",
    "        return np.array(ast.literal_eval(embedding_str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Load BERT, Word2Vec, and GloVe embeddings CSV files\n",
    "bert_word2vec_df = pd.read_csv(r'B:\\OneDrive - Amity University\\Desktop\\Tower Research\\Modify\\modified_embeddings_bert_word2vec.csv')\n",
    "glove_df = pd.read_csv(r'B:\\OneDrive - Amity University\\Desktop\\Tower Research\\Modify\\modified_embeddings_glove.csv')\n",
    "\n",
    "# Convert the 'decision' column into binary labels\n",
    "decision_mapping = {'select': 1, 'Select': 1, 'selected': 1, 'rejected': 0, 'Reject': 0, 'reject': 0}\n",
    "bert_word2vec_df['decision1'] = bert_word2vec_df['decision'].map(decision_mapping)\n",
    "glove_df['decision1'] = glove_df['decision'].map(decision_mapping)\n",
    "\n",
    "# Process embedding columns in both dataframes\n",
    "embedding_columns_bert = bert_word2vec_df.columns.difference(['decision', 'decision1', 'id', 'name', 'role', 'transcript', 'resume', 'reason_for_decision', 'job_description', 'source_file', 'source_sheet'])\n",
    "embedding_columns_glove = glove_df.columns.difference(['decision', 'decision1', 'id', 'name', 'role', 'transcript', 'resume', 'reason_for_decision', 'job_description', 'source_file', 'source_sheet'])\n",
    "\n",
    "# Convert embedding strings to numpy arrays\n",
    "for col in embedding_columns_bert:\n",
    "    bert_word2vec_df[col] = bert_word2vec_df[col].apply(process_embedding_string)\n",
    "for col in embedding_columns_glove:\n",
    "    glove_df[col] = glove_df[col].apply(process_embedding_string)\n",
    "\n",
    "# Create feature matrix by concatenating all embedding vectors\n",
    "def create_feature_matrix(df, embedding_columns):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        combined_embedding = np.concatenate([row[col] for col in embedding_columns if row[col] is not None])\n",
    "        features.append(combined_embedding)\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Create feature matrices\n",
    "X_bert_word2vec = create_feature_matrix(bert_word2vec_df, embedding_columns_bert)\n",
    "X_glove = create_feature_matrix(glove_df, embedding_columns_glove)\n",
    "\n",
    "# Combine all features\n",
    "X = np.hstack([X_bert_word2vec, X_glove])\n",
    "y = bert_word2vec_df['decision1'].values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train ANN Model\n",
    "def train_ann(X_train, y_train, X_test, y_test):\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(64, activation='relu'))\n",
    "    ann_model.add(Dropout(0.2))\n",
    "    ann_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    ann_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    ann_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    y_pred_ann = (ann_model.predict(X_test) > 0.5).astype(int)\n",
    "    ann_accuracy = accuracy_score(y_test, y_pred_ann)\n",
    "    print(f'ANN Model Accuracy: {ann_accuracy:.4f}')\n",
    "    return ann_model\n",
    "\n",
    "# Train XGBoost Model\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f'XGBoost Model Accuracy: {xgb_accuracy:.4f}')\n",
    "    return xgb_model\n",
    "\n",
    "# Train models\n",
    "print(\"Training ANN model...\")\n",
    "ann_model = train_ann(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model = train_xgboost(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Save models\n",
    "ann_model.save('ann_model.h5')\n",
    "xgb_model.save_model('xgb_model.json')\n",
    "joblib.dump(ann_model, 'ann_model.pkl')\n",
    "\n",
    "print(\"\\nModels saved successfully as 'ann_model.h5', 'xgb_model.json', and 'ann_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre Proccessing the File for further work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          name               role  \\\n",
      "0  brenbr359   Brent Brown    Product Manager   \n",
      "1  jameay305   James Ayala  Software Engineer   \n",
      "2  scotri565  Scott Rivera      Data Engineer   \n",
      "3  emilke232   Emily Kelly        UI Engineer   \n",
      "4  ashlra638    Ashley Ray     Data Scientist   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
      "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
      "2  Here is a simulated interview for Scott Rivera...   \n",
      "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
      "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
      "\n",
      "                                              resume decision  \\\n",
      "0  Here's a sample resume for Brent Brown applyin...   select   \n",
      "1  Here's a sample resume for James Ayala applyin...   select   \n",
      "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
      "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
      "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
      "\n",
      "  reason_for_decision                                    job_description  \\\n",
      "0          Experience  We are looking for a skilled Product Manager w...   \n",
      "1          Experience  We are looking for a skilled Software Engineer...   \n",
      "2          Experience  We are looking for a skilled Data Engineer wit...   \n",
      "3          Experience  We are looking for a skilled UI Engineer with ...   \n",
      "4        Cultural Fit  We are looking for a skilled Data Scientist wi...   \n",
      "\n",
      "   Unnamed: 8  Unnamed: 9  ... transcript_job_keyword_overlap role_popularity  \\\n",
      "0         NaN         NaN  ...                             10             514   \n",
      "1         NaN         NaN  ...                             10             551   \n",
      "2         NaN         NaN  ...                              7             509   \n",
      "3         NaN         NaN  ...                              8             291   \n",
      "4         NaN         NaN  ...                              9             592   \n",
      "\n",
      "  decision_reason_encoded Unnamed: 45 resume_job_similarity  \\\n",
      "0                      54         NaN                   1.0   \n",
      "1                      54         NaN                   1.0   \n",
      "2                      54         NaN                   1.0   \n",
      "3                      54         NaN                   1.0   \n",
      "4                       5         NaN                   1.0   \n",
      "\n",
      "  transcript_job_similarity  transcript_resume_similarity  \\\n",
      "0                       1.0                           1.0   \n",
      "1                       1.0                           1.0   \n",
      "2                       1.0                           1.0   \n",
      "3                       1.0                           1.0   \n",
      "4                       1.0                           1.0   \n",
      "\n",
      "   job_description_embedding_glove  transcript_embedding_glove  \\\n",
      "0                               []                          []   \n",
      "1                               []                          []   \n",
      "2                               []                          []   \n",
      "3                               []                          []   \n",
      "4                               []                          []   \n",
      "\n",
      "   resume_embedding_glove  \n",
      "0                      []  \n",
      "1                      []  \n",
      "2                      []  \n",
      "3                      []  \n",
      "4                      []  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(r'B:\\OneDrive - Amity University\\Desktop\\Assignment-5\\glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx')\n",
    "\n",
    "\n",
    "embedding_columns = ['job_description_bert', 'transcript_bert', 'resume_bert', \n",
    "                     'job_description_word2vec', 'transcript_word2vec', 'resume_word2vec', \n",
    "                     'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove']\n",
    "\n",
    "# Function to convert the string of numbers into a list of floats\n",
    "def convert_to_float(embedding_str):\n",
    "    if pd.isnull(embedding_str) or embedding_str == '':\n",
    "        return []  # or np.nan depending on your preferred handling\n",
    "    try:\n",
    "        embedding_list = embedding_str.strip('[]').split()  # remove the brackets and split the numbers\n",
    "        return [float(num) for num in embedding_list]  # convert each number to float\n",
    "    except ValueError:\n",
    "        return []  # or np.nan if conversion fails\n",
    "\n",
    "# Apply the function to the relevant columns\n",
    "for col in embedding_columns:\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "\n",
    "# Optionally, check the first few rows of the DataFrame to confirm\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new Excel file\n",
    "df.to_excel(r'pre_glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training XG BOOST and ANN on EDA FEATURES with BERT,GLOVE AND WORD2VEC EMBEDSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Enhanced ANN model...\n",
      "Epoch 1/200\n",
      "21/21 [==============================] - 3s 41ms/step - loss: 12.1819 - accuracy: 0.7254 - val_loss: 10.8988 - val_accuracy: 0.7831 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 9.6010 - accuracy: 0.7838 - val_loss: 8.4789 - val_accuracy: 0.8092 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 7.2842 - accuracy: 0.8012 - val_loss: 6.4488 - val_accuracy: 0.8092 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 5.4609 - accuracy: 0.8046 - val_loss: 4.9014 - val_accuracy: 0.8092 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 4.0838 - accuracy: 0.8165 - val_loss: 3.7666 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 3.0849 - accuracy: 0.8169 - val_loss: 2.9584 - val_accuracy: 0.6969 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.3831 - accuracy: 0.8104 - val_loss: 2.3724 - val_accuracy: 0.6569 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.8562 - accuracy: 0.8219 - val_loss: 1.9381 - val_accuracy: 0.6338 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.4957 - accuracy: 0.8188 - val_loss: 1.6251 - val_accuracy: 0.7477 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2636 - accuracy: 0.7977 - val_loss: 1.4233 - val_accuracy: 0.7769 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0597 - accuracy: 0.8131 - val_loss: 1.2624 - val_accuracy: 0.7862 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.9027 - accuracy: 0.8115 - val_loss: 1.1361 - val_accuracy: 0.6138 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.7892 - accuracy: 0.8169 - val_loss: 1.0305 - val_accuracy: 0.7815 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6964 - accuracy: 0.8238 - val_loss: 0.9505 - val_accuracy: 0.7954 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.6522 - accuracy: 0.8250 - val_loss: 0.8941 - val_accuracy: 0.7538 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6479 - accuracy: 0.8092 - val_loss: 0.8268 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6228 - accuracy: 0.8138 - val_loss: 0.7311 - val_accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.5696 - accuracy: 0.8192 - val_loss: 0.6950 - val_accuracy: 0.8077 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.5278 - accuracy: 0.8246 - val_loss: 0.6912 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.5075 - accuracy: 0.8250 - val_loss: 0.7005 - val_accuracy: 0.7769 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5130 - accuracy: 0.8231 - val_loss: 0.6285 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.5192 - accuracy: 0.8123 - val_loss: 0.6056 - val_accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5102 - accuracy: 0.8192 - val_loss: 0.5586 - val_accuracy: 0.8015 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4996 - accuracy: 0.8127 - val_loss: 0.5623 - val_accuracy: 0.8062 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4918 - accuracy: 0.8204 - val_loss: 0.5400 - val_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4586 - accuracy: 0.8285 - val_loss: 0.5434 - val_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4416 - accuracy: 0.8231 - val_loss: 0.5744 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.4534 - accuracy: 0.8212 - val_loss: 0.5076 - val_accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4727 - accuracy: 0.8069 - val_loss: 0.5180 - val_accuracy: 0.8154 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4687 - accuracy: 0.8081 - val_loss: 0.4866 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4741 - accuracy: 0.8250 - val_loss: 0.4997 - val_accuracy: 0.7862 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4745 - accuracy: 0.8108 - val_loss: 0.4805 - val_accuracy: 0.8062 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4716 - accuracy: 0.8127 - val_loss: 0.5054 - val_accuracy: 0.8077 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4758 - accuracy: 0.8188 - val_loss: 0.4895 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4562 - accuracy: 0.8162 - val_loss: 0.4699 - val_accuracy: 0.7815 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4592 - accuracy: 0.8115 - val_loss: 0.4681 - val_accuracy: 0.8062 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4409 - accuracy: 0.8308 - val_loss: 0.4691 - val_accuracy: 0.8046 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4557 - accuracy: 0.8146 - val_loss: 0.4931 - val_accuracy: 0.7969 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4587 - accuracy: 0.8204 - val_loss: 0.4660 - val_accuracy: 0.7862 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4512 - accuracy: 0.8235 - val_loss: 0.4673 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4501 - accuracy: 0.8227 - val_loss: 0.4787 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.4556 - accuracy: 0.8069 - val_loss: 0.4784 - val_accuracy: 0.7969 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4591 - accuracy: 0.8196 - val_loss: 0.4786 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4613 - accuracy: 0.8232\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.4636 - accuracy: 0.8215 - val_loss: 0.4934 - val_accuracy: 0.7923 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4574 - accuracy: 0.8292 - val_loss: 0.4787 - val_accuracy: 0.7938 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4388 - accuracy: 0.8265 - val_loss: 0.4545 - val_accuracy: 0.7938 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4116 - accuracy: 0.8323 - val_loss: 0.4568 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4134 - accuracy: 0.8238 - val_loss: 0.4535 - val_accuracy: 0.8015 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4064 - accuracy: 0.8296 - val_loss: 0.4577 - val_accuracy: 0.7969 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4067 - accuracy: 0.8277 - val_loss: 0.4524 - val_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4015 - accuracy: 0.8292 - val_loss: 0.4421 - val_accuracy: 0.8108 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4370 - accuracy: 0.8108 - val_loss: 0.4340 - val_accuracy: 0.8108 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4187 - accuracy: 0.8246 - val_loss: 0.4430 - val_accuracy: 0.8015 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4100 - accuracy: 0.8319 - val_loss: 0.4259 - val_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4199 - accuracy: 0.8262 - val_loss: 0.4333 - val_accuracy: 0.8108 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4080 - accuracy: 0.8250 - val_loss: 0.4292 - val_accuracy: 0.8046 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4095 - accuracy: 0.8262 - val_loss: 0.4343 - val_accuracy: 0.8015 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4039 - accuracy: 0.8338 - val_loss: 0.4362 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3944 - accuracy: 0.8338 - val_loss: 0.4250 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3980 - accuracy: 0.8227 - val_loss: 0.4217 - val_accuracy: 0.8077 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4172 - accuracy: 0.8196 - val_loss: 0.4506 - val_accuracy: 0.8046 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4221 - accuracy: 0.8192 - val_loss: 0.4249 - val_accuracy: 0.8046 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4070 - accuracy: 0.8327 - val_loss: 0.4333 - val_accuracy: 0.7954 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3959 - accuracy: 0.8312 - val_loss: 0.4496 - val_accuracy: 0.7969 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4183 - accuracy: 0.8174\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4196 - accuracy: 0.8150 - val_loss: 0.4265 - val_accuracy: 0.7985 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4016 - accuracy: 0.8358 - val_loss: 0.4205 - val_accuracy: 0.8031 - lr: 2.5000e-04\n",
      "Epoch 67/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3843 - accuracy: 0.8392 - val_loss: 0.4142 - val_accuracy: 0.8108 - lr: 2.5000e-04\n",
      "Epoch 68/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3792 - accuracy: 0.8412 - val_loss: 0.4129 - val_accuracy: 0.8015 - lr: 2.5000e-04\n",
      "Epoch 69/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3818 - accuracy: 0.8304 - val_loss: 0.4171 - val_accuracy: 0.8092 - lr: 2.5000e-04\n",
      "Epoch 70/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3726 - accuracy: 0.8365 - val_loss: 0.4119 - val_accuracy: 0.7985 - lr: 2.5000e-04\n",
      "Epoch 71/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3796 - accuracy: 0.8331 - val_loss: 0.4144 - val_accuracy: 0.8077 - lr: 2.5000e-04\n",
      "Epoch 72/200\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3696 - accuracy: 0.8335 - val_loss: 0.4103 - val_accuracy: 0.8123 - lr: 2.5000e-04\n",
      "Epoch 73/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3755 - accuracy: 0.8235 - val_loss: 0.4145 - val_accuracy: 0.7969 - lr: 2.5000e-04\n",
      "Epoch 74/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3649 - accuracy: 0.8381 - val_loss: 0.4015 - val_accuracy: 0.8031 - lr: 2.5000e-04\n",
      "Epoch 75/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3687 - accuracy: 0.8327 - val_loss: 0.4062 - val_accuracy: 0.7969 - lr: 2.5000e-04\n",
      "Epoch 76/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3683 - accuracy: 0.8354 - val_loss: 0.4084 - val_accuracy: 0.8077 - lr: 2.5000e-04\n",
      "Epoch 77/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3599 - accuracy: 0.8419 - val_loss: 0.4025 - val_accuracy: 0.8046 - lr: 2.5000e-04\n",
      "Epoch 78/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3628 - accuracy: 0.8288 - val_loss: 0.3896 - val_accuracy: 0.8077 - lr: 2.5000e-04\n",
      "Epoch 79/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3617 - accuracy: 0.8304 - val_loss: 0.4019 - val_accuracy: 0.8046 - lr: 2.5000e-04\n",
      "Epoch 80/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3659 - accuracy: 0.8365 - val_loss: 0.3901 - val_accuracy: 0.8092 - lr: 2.5000e-04\n",
      "Epoch 81/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3572 - accuracy: 0.8338 - val_loss: 0.3978 - val_accuracy: 0.8000 - lr: 2.5000e-04\n",
      "Epoch 82/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3666 - accuracy: 0.8269 - val_loss: 0.3938 - val_accuracy: 0.8031 - lr: 2.5000e-04\n",
      "Epoch 83/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3757 - accuracy: 0.8252\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3710 - accuracy: 0.8292 - val_loss: 0.3972 - val_accuracy: 0.8046 - lr: 2.5000e-04\n",
      "Epoch 84/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3607 - accuracy: 0.8381 - val_loss: 0.3948 - val_accuracy: 0.8015 - lr: 1.2500e-04\n",
      "Epoch 85/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3511 - accuracy: 0.8496 - val_loss: 0.3859 - val_accuracy: 0.8123 - lr: 1.2500e-04\n",
      "Epoch 86/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3477 - accuracy: 0.8427 - val_loss: 0.3908 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
      "Epoch 87/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3494 - accuracy: 0.8412 - val_loss: 0.3871 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
      "Epoch 88/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3516 - accuracy: 0.8358 - val_loss: 0.3862 - val_accuracy: 0.8046 - lr: 1.2500e-04\n",
      "Epoch 89/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3557 - accuracy: 0.8242 - val_loss: 0.3890 - val_accuracy: 0.8062 - lr: 1.2500e-04\n",
      "Epoch 90/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3461 - accuracy: 0.8450 - val_loss: 0.3842 - val_accuracy: 0.8062 - lr: 1.2500e-04\n",
      "Epoch 91/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3489 - accuracy: 0.8319 - val_loss: 0.3876 - val_accuracy: 0.7923 - lr: 1.2500e-04\n",
      "Epoch 92/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3500 - accuracy: 0.8373 - val_loss: 0.3847 - val_accuracy: 0.8046 - lr: 1.2500e-04\n",
      "Epoch 93/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3547 - accuracy: 0.8192 - val_loss: 0.3828 - val_accuracy: 0.8077 - lr: 1.2500e-04\n",
      "Epoch 94/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3422 - accuracy: 0.8404 - val_loss: 0.3836 - val_accuracy: 0.8015 - lr: 1.2500e-04\n",
      "Epoch 95/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3477 - accuracy: 0.8335 - val_loss: 0.3798 - val_accuracy: 0.8046 - lr: 1.2500e-04\n",
      "Epoch 96/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3347 - accuracy: 0.8485 - val_loss: 0.3909 - val_accuracy: 0.7969 - lr: 1.2500e-04\n",
      "Epoch 97/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3351 - accuracy: 0.8381 - val_loss: 0.3793 - val_accuracy: 0.8123 - lr: 1.2500e-04\n",
      "Epoch 98/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3453 - accuracy: 0.8342 - val_loss: 0.3817 - val_accuracy: 0.8077 - lr: 1.2500e-04\n",
      "Epoch 99/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3389 - accuracy: 0.8454 - val_loss: 0.3796 - val_accuracy: 0.8108 - lr: 1.2500e-04\n",
      "Epoch 100/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3398 - accuracy: 0.8485 - val_loss: 0.3760 - val_accuracy: 0.8108 - lr: 1.2500e-04\n",
      "Epoch 101/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3552 - accuracy: 0.8327 - val_loss: 0.3822 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
      "Epoch 102/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3326 - accuracy: 0.8458 - val_loss: 0.3864 - val_accuracy: 0.8000 - lr: 1.2500e-04\n",
      "Epoch 103/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3364 - accuracy: 0.8496 - val_loss: 0.3819 - val_accuracy: 0.7985 - lr: 1.2500e-04\n",
      "Epoch 104/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3421 - accuracy: 0.8392 - val_loss: 0.3759 - val_accuracy: 0.8031 - lr: 1.2500e-04\n",
      "Epoch 105/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3482 - accuracy: 0.8318\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3505 - accuracy: 0.8315 - val_loss: 0.3831 - val_accuracy: 0.7969 - lr: 1.2500e-04\n",
      "Epoch 106/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3363 - accuracy: 0.8412 - val_loss: 0.3807 - val_accuracy: 0.8062 - lr: 6.2500e-05\n",
      "Epoch 107/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3495 - accuracy: 0.8254 - val_loss: 0.3754 - val_accuracy: 0.8138 - lr: 6.2500e-05\n",
      "Epoch 108/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3346 - accuracy: 0.8338 - val_loss: 0.3766 - val_accuracy: 0.8077 - lr: 6.2500e-05\n",
      "Epoch 109/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3335 - accuracy: 0.8412 - val_loss: 0.3770 - val_accuracy: 0.8138 - lr: 6.2500e-05\n",
      "Epoch 110/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3282 - accuracy: 0.8488 - val_loss: 0.3726 - val_accuracy: 0.8138 - lr: 6.2500e-05\n",
      "Epoch 111/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3276 - accuracy: 0.8469 - val_loss: 0.3764 - val_accuracy: 0.8123 - lr: 6.2500e-05\n",
      "Epoch 112/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3349 - accuracy: 0.8396 - val_loss: 0.3743 - val_accuracy: 0.7985 - lr: 6.2500e-05\n",
      "Epoch 113/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3290 - accuracy: 0.8465 - val_loss: 0.3766 - val_accuracy: 0.8046 - lr: 6.2500e-05\n",
      "Epoch 114/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3282 - accuracy: 0.8477 - val_loss: 0.3721 - val_accuracy: 0.8000 - lr: 6.2500e-05\n",
      "Epoch 115/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3238 - accuracy: 0.8446 - val_loss: 0.3750 - val_accuracy: 0.8169 - lr: 6.2500e-05\n",
      "Epoch 116/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3282 - accuracy: 0.8369 - val_loss: 0.3711 - val_accuracy: 0.8077 - lr: 6.2500e-05\n",
      "Epoch 117/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3306 - accuracy: 0.8438 - val_loss: 0.3706 - val_accuracy: 0.8092 - lr: 6.2500e-05\n",
      "Epoch 118/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3354 - accuracy: 0.8369 - val_loss: 0.3709 - val_accuracy: 0.8031 - lr: 6.2500e-05\n",
      "Epoch 119/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3292 - accuracy: 0.8423 - val_loss: 0.3686 - val_accuracy: 0.7969 - lr: 6.2500e-05\n",
      "Epoch 120/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3299 - accuracy: 0.8404 - val_loss: 0.3685 - val_accuracy: 0.8092 - lr: 6.2500e-05\n",
      "Epoch 121/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3272 - accuracy: 0.8431 - val_loss: 0.3701 - val_accuracy: 0.8077 - lr: 6.2500e-05\n",
      "Epoch 122/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3268 - accuracy: 0.8462 - val_loss: 0.3740 - val_accuracy: 0.8062 - lr: 6.2500e-05\n",
      "Epoch 123/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3280 - accuracy: 0.8373 - val_loss: 0.3737 - val_accuracy: 0.7985 - lr: 6.2500e-05\n",
      "Epoch 124/200\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3335 - accuracy: 0.8365 - val_loss: 0.3708 - val_accuracy: 0.8000 - lr: 6.2500e-05\n",
      "Epoch 125/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3320 - accuracy: 0.8376\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3317 - accuracy: 0.8381 - val_loss: 0.3690 - val_accuracy: 0.7985 - lr: 6.2500e-05\n",
      "Epoch 126/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3314 - accuracy: 0.8435 - val_loss: 0.3726 - val_accuracy: 0.8000 - lr: 3.1250e-05\n",
      "Epoch 127/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3258 - accuracy: 0.8442 - val_loss: 0.3710 - val_accuracy: 0.8062 - lr: 3.1250e-05\n",
      "Epoch 128/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3252 - accuracy: 0.8481 - val_loss: 0.3680 - val_accuracy: 0.7938 - lr: 3.1250e-05\n",
      "Epoch 129/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3192 - accuracy: 0.8465 - val_loss: 0.3692 - val_accuracy: 0.8031 - lr: 3.1250e-05\n",
      "Epoch 130/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3204 - accuracy: 0.8465 - val_loss: 0.3690 - val_accuracy: 0.8077 - lr: 3.1250e-05\n",
      "Epoch 131/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3255 - accuracy: 0.8412 - val_loss: 0.3685 - val_accuracy: 0.8108 - lr: 3.1250e-05\n",
      "Epoch 132/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3302 - accuracy: 0.8346 - val_loss: 0.3678 - val_accuracy: 0.8000 - lr: 3.1250e-05\n",
      "Epoch 133/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3283 - accuracy: 0.8438 - val_loss: 0.3695 - val_accuracy: 0.7938 - lr: 3.1250e-05\n",
      "Epoch 134/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3292 - accuracy: 0.8438 - val_loss: 0.3670 - val_accuracy: 0.8077 - lr: 3.1250e-05\n",
      "Epoch 135/200\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3210 - accuracy: 0.8454 - val_loss: 0.3661 - val_accuracy: 0.8108 - lr: 3.1250e-05\n",
      "Epoch 136/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3260 - accuracy: 0.8354 - val_loss: 0.3660 - val_accuracy: 0.8046 - lr: 3.1250e-05\n",
      "Epoch 137/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3224 - accuracy: 0.8458 - val_loss: 0.3670 - val_accuracy: 0.8077 - lr: 3.1250e-05\n",
      "Epoch 138/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3192 - accuracy: 0.8477 - val_loss: 0.3695 - val_accuracy: 0.8015 - lr: 3.1250e-05\n",
      "Epoch 139/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3158 - accuracy: 0.8569 - val_loss: 0.3680 - val_accuracy: 0.8015 - lr: 3.1250e-05\n",
      "Epoch 140/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3211 - accuracy: 0.8446 - val_loss: 0.3679 - val_accuracy: 0.8015 - lr: 3.1250e-05\n",
      "Epoch 141/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3165 - accuracy: 0.8470\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3167 - accuracy: 0.8458 - val_loss: 0.3674 - val_accuracy: 0.8077 - lr: 3.1250e-05\n",
      "Epoch 142/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3226 - accuracy: 0.8373 - val_loss: 0.3675 - val_accuracy: 0.8046 - lr: 1.5625e-05\n",
      "Epoch 143/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3148 - accuracy: 0.8469 - val_loss: 0.3671 - val_accuracy: 0.8062 - lr: 1.5625e-05\n",
      "Epoch 144/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3227 - accuracy: 0.8377 - val_loss: 0.3661 - val_accuracy: 0.8108 - lr: 1.5625e-05\n",
      "Epoch 145/200\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3172 - accuracy: 0.8377 - val_loss: 0.3649 - val_accuracy: 0.8108 - lr: 1.5625e-05\n",
      "Epoch 146/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3178 - accuracy: 0.8462 - val_loss: 0.3656 - val_accuracy: 0.8138 - lr: 1.5625e-05\n",
      "Epoch 147/200\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.3186 - accuracy: 0.8388 - val_loss: 0.3657 - val_accuracy: 0.8123 - lr: 1.5625e-05\n",
      "Epoch 148/200\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3161 - accuracy: 0.8423 - val_loss: 0.3653 - val_accuracy: 0.8123 - lr: 1.5625e-05\n",
      "Epoch 149/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3183 - accuracy: 0.8423 - val_loss: 0.3632 - val_accuracy: 0.8092 - lr: 1.5625e-05\n",
      "Epoch 150/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3106 - accuracy: 0.8527 - val_loss: 0.3642 - val_accuracy: 0.8000 - lr: 1.5625e-05\n",
      "Epoch 151/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3180 - accuracy: 0.8419 - val_loss: 0.3636 - val_accuracy: 0.8077 - lr: 1.5625e-05\n",
      "Epoch 152/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3267 - accuracy: 0.8358 - val_loss: 0.3654 - val_accuracy: 0.8046 - lr: 1.5625e-05\n",
      "Epoch 153/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3234 - accuracy: 0.8423 - val_loss: 0.3625 - val_accuracy: 0.8062 - lr: 1.5625e-05\n",
      "Epoch 154/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3170 - accuracy: 0.8454 - val_loss: 0.3638 - val_accuracy: 0.8031 - lr: 1.5625e-05\n",
      "Epoch 155/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3094 - accuracy: 0.8512 - val_loss: 0.3640 - val_accuracy: 0.8092 - lr: 1.5625e-05\n",
      "Epoch 156/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3246 - accuracy: 0.8373 - val_loss: 0.3622 - val_accuracy: 0.8092 - lr: 1.5625e-05\n",
      "Epoch 157/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3152 - accuracy: 0.8462 - val_loss: 0.3650 - val_accuracy: 0.8092 - lr: 1.5625e-05\n",
      "Epoch 158/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3202 - accuracy: 0.8438 - val_loss: 0.3635 - val_accuracy: 0.8123 - lr: 1.5625e-05\n",
      "Epoch 159/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3172 - accuracy: 0.8462 - val_loss: 0.3655 - val_accuracy: 0.8046 - lr: 1.5625e-05\n",
      "Epoch 160/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3176 - accuracy: 0.8404 - val_loss: 0.3655 - val_accuracy: 0.8031 - lr: 1.5625e-05\n",
      "Epoch 161/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3118 - accuracy: 0.8487\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3118 - accuracy: 0.8485 - val_loss: 0.3655 - val_accuracy: 0.8046 - lr: 1.5625e-05\n",
      "Epoch 162/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3209 - accuracy: 0.8404 - val_loss: 0.3656 - val_accuracy: 0.8046 - lr: 7.8125e-06\n",
      "Epoch 163/200\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3165 - accuracy: 0.8477 - val_loss: 0.3657 - val_accuracy: 0.8046 - lr: 7.8125e-06\n",
      "Epoch 164/200\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3133 - accuracy: 0.8523 - val_loss: 0.3651 - val_accuracy: 0.8123 - lr: 7.8125e-06\n",
      "Epoch 165/200\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3095 - accuracy: 0.8512 - val_loss: 0.3644 - val_accuracy: 0.8092 - lr: 7.8125e-06\n",
      "Epoch 166/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3098 - accuracy: 0.8549\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3141 - accuracy: 0.8523 - val_loss: 0.3639 - val_accuracy: 0.8108 - lr: 7.8125e-06\n",
      "Epoch 167/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3240 - accuracy: 0.8388 - val_loss: 0.3641 - val_accuracy: 0.8077 - lr: 3.9063e-06\n",
      "Epoch 168/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3168 - accuracy: 0.8427 - val_loss: 0.3635 - val_accuracy: 0.8108 - lr: 3.9063e-06\n",
      "Epoch 169/200\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3147 - accuracy: 0.8423 - val_loss: 0.3637 - val_accuracy: 0.8092 - lr: 3.9063e-06\n",
      "Epoch 170/200\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3195 - accuracy: 0.8446 - val_loss: 0.3637 - val_accuracy: 0.8062 - lr: 3.9063e-06\n",
      "Epoch 171/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3177 - accuracy: 0.8371\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3181 - accuracy: 0.8369 - val_loss: 0.3632 - val_accuracy: 0.8062 - lr: 3.9063e-06\n",
      "21/21 [==============================] - 0s 4ms/step\n",
      "Advanced ANN Model Accuracy: 0.8092\n",
      "\n",
      "Training Enhanced XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "C:\\Users\\sidhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [20:42:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex XGBoost Model Accuracy: 0.8492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler1.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import ast\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Path to your dataset\n",
    "final_path_file = r'B:\\OneDrive - Amity University\\Desktop\\Assignment-5\\glove_word2vec_embss_bert_embedss_EDA_dataset.xlsx'\n",
    "combine_df_final = pd.read_excel(final_path_file)\n",
    "\n",
    "# Mapping for decision encoding\n",
    "decision_mapping_final = {'select': 1, 'Select': 1, 'selected': 1, 'rejected': 0, 'Reject': 0, 'reject': 0}\n",
    "combine_df_final['decision_encoded'] = combine_df_final['decision'].map(decision_mapping_final)\n",
    "\n",
    "# Embedding and other feature columns\n",
    "features = [\n",
    "    'job_description_bert', 'transcript_bert', 'resume_bert', 'job_description_word2vec', 'transcript_word2vec',\n",
    "    'resume_word2vec', 'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove',\n",
    "    'resume_word_count', 'resume_char_count', 'resume_avg_word_length', 'resume_sentence_count',\n",
    "    'resume_uppercase_ratio', 'resume_technical_keyword_count', 'resume_positive_keyword_count',\n",
    "    'resume_negative_keyword_count', 'resume_unique_word_ratio', 'transcript_word_count',\n",
    "    'transcript_char_count', 'transcript_avg_word_length', 'transcript_sentence_count',\n",
    "    'transcript_uppercase_ratio', 'transcript_positive_keyword_count', 'transcript_negative_keyword_count',\n",
    "    'transcript_unique_word_ratio', 'job_role_in_resume', 'resume_job_keyword_overlap',\n",
    "    'transcript_job_keyword_overlap', 'role_popularity', 'decision_reason_encoded',\n",
    "    'resume_job_similarity', 'transcript_job_similarity', 'transcript_resume_similarity'\n",
    "]\n",
    "target = 'decision_encoded'\n",
    "\n",
    "# Convert string representations of lists into actual lists\n",
    "def convert_to_list(value):\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Apply the conversion function to embedding columns\n",
    "embedding_columns = [\n",
    "    'job_description_bert', 'transcript_bert', 'resume_bert',\n",
    "    'job_description_word2vec', 'transcript_word2vec', 'resume_word2vec',\n",
    "    'job_description_embedding_glove', 'transcript_embedding_glove', 'resume_embedding_glove'\n",
    "]\n",
    "\n",
    "for col in embedding_columns:\n",
    "    combine_df_final[col] = combine_df_final[col].apply(convert_to_list)\n",
    "\n",
    "# Flatten embedding columns to a single numeric value (e.g., by taking the mean of the list)\n",
    "def flatten_embedding_column(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(lambda x: np.mean(x) if isinstance(x, list) else 0)\n",
    "\n",
    "for col in embedding_columns:\n",
    "    flatten_embedding_column(combine_df_final, col)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = combine_df_final[features]\n",
    "y = combine_df_final[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Enhanced ANN Model\n",
    "def train_advanced_ann(X_train, y_train, X_test, y_test):\n",
    "    ann_model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    ann_model.add(Dense(1024, input_dim=X_train.shape[1], kernel_regularizer=l2(0.01)))\n",
    "    ann_model.add(Activation('swish'))\n",
    "    ann_model.add(BatchNormalization())\n",
    "    ann_model.add(Dropout(0.5))\n",
    "\n",
    "    # Hidden Layer 1\n",
    "    ann_model.add(Dense(512, kernel_regularizer=l2(0.01)))\n",
    "    ann_model.add(Activation('swish'))\n",
    "    ann_model.add(BatchNormalization())\n",
    "    ann_model.add(Dropout(0.5))\n",
    "\n",
    "    # Hidden Layer 2\n",
    "    ann_model.add(Dense(256, kernel_regularizer=l2(0.01)))\n",
    "    ann_model.add(Activation('swish'))\n",
    "    ann_model.add(BatchNormalization())\n",
    "    ann_model.add(Dropout(0.4))\n",
    "\n",
    "    # Hidden Layer 3\n",
    "    ann_model.add(Dense(128, kernel_regularizer=l2(0.01)))\n",
    "    ann_model.add(Activation('swish'))\n",
    "    ann_model.add(BatchNormalization())\n",
    "    ann_model.add(Dropout(0.3))\n",
    "\n",
    "    # Output Layer\n",
    "    ann_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer with Learning Rate Scheduling\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    # Compile the Model\n",
    "    ann_model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train the Model\n",
    "    ann_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=128,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the Model\n",
    "    y_pred_ann = (ann_model.predict(X_test) > 0.5).astype(int)\n",
    "    ann_accuracy = accuracy_score(y_test, y_pred_ann)\n",
    "    print(f'Advanced ANN Model Accuracy: {ann_accuracy:.4f}')\n",
    "\n",
    "    return ann_model\n",
    "\n",
    "# Enhanced XGBoost Model\n",
    "def train_xgboost_complex(X_train, y_train, X_test, y_test):\n",
    "    params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0.01, 0.1, 1.0],\n",
    "        'reg_lambda': [0.01, 0.1, 1.0],\n",
    "    }\n",
    "    xgb_model = xgb.XGBClassifier(tree_method='hist', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=params, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f'Complex XGBoost Model Accuracy: {xgb_accuracy:.4f}')\n",
    "    return best_xgb_model\n",
    "\n",
    "# Train and save the models\n",
    "print(\"Training Enhanced ANN model...\")\n",
    "ann_model = train_advanced_ann(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "ann_model.save('complex_ann_model1.h5')\n",
    "\n",
    "print(\"\\nTraining Enhanced XGBoost model...\")\n",
    "xgb_model = train_xgboost_complex(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "xgb_model.save_model('complex_xgb_model1.json')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler1.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine and comparing the probab of both models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 4ms/step\n",
      "ANN Model Accuracy: 0.8092, AUC: 0.9199\n",
      "XGBoost Model Accuracy: 0.8492, AUC: 0.9448\n",
      "Combined Model Accuracy: 0.8308, AUC: 0.9401\n",
      "ANN Model Accuracy: 0.8092, AUC: 0.9199\n",
      "XGBoost Model Accuracy: 0.8492, AUC: 0.9448\n",
      "Combined Model Accuracy: 0.8308, AUC: 0.9401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "# ANN model probabilities\n",
    "ann_probs = ann_model.predict(X_test_scaled)\n",
    "\n",
    "# XGBoost model probabilities\n",
    "xgb_probs = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 2: Combine Probabilities\n",
    "combined_probs = (ann_probs.flatten() + xgb_probs) / 2\n",
    "\n",
    "# Step 3: Generate Final Predictions\n",
    "combined_preds = (combined_probs > 0.5).astype(int)\n",
    "\n",
    "# Step 4: Evaluate Performance\n",
    "# Individual Model Accuracies\n",
    "ann_accuracy = accuracy_score(y_test, (ann_probs > 0.5).astype(int))\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_model.predict(X_test_scaled))\n",
    "\n",
    "# Combined Model Accuracy\n",
    "combined_accuracy = accuracy_score(y_test, combined_preds)\n",
    "\n",
    "# AUC Scores\n",
    "ann_auc = roc_auc_score(y_test, ann_probs)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
    "combined_auc = roc_auc_score(y_test, combined_probs)\n",
    "\n",
    "# Print Results\n",
    "print(f\"ANN Model Accuracy: {ann_accuracy:.4f}, AUC: {ann_auc:.4f}\")\n",
    "print(f\"XGBoost Model Accuracy: {xgb_accuracy:.4f}, AUC: {xgb_auc:.4f}\")\n",
    "print(f\"Combined Model Accuracy: {combined_accuracy:.4f}, AUC: {combined_auc:.4f}\")\n",
    "\n",
    "# Step 3: Generate Final Predictions\n",
    "combined_preds = (combined_probs > 0.5).astype(int)\n",
    "\n",
    "# Step 4: Evaluate Performance\n",
    "# Individual Model Accuracies\n",
    "ann_accuracy = accuracy_score(y_test, (ann_probs > 0.5).astype(int))\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_model.predict(X_test_scaled))\n",
    "\n",
    "# Combined Model Accuracy\n",
    "combined_accuracy = accuracy_score(y_test, combined_preds)\n",
    "\n",
    "# AUC Scores\n",
    "ann_auc = roc_auc_score(y_test, ann_probs)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
    "combined_auc = roc_auc_score(y_test, combined_probs)\n",
    "\n",
    "# Print Results\n",
    "print(f\"ANN Model Accuracy: {ann_accuracy:.4f}, AUC: {ann_auc:.4f}\")\n",
    "print(f\"XGBoost Model Accuracy: {xgb_accuracy:.4f}, AUC: {xgb_auc:.4f}\")\n",
    "print(f\"Combined Model Accuracy: {combined_accuracy:.4f}, AUC: {combined_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
